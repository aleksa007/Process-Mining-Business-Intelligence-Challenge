{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree predicting the next event of a case - Full Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import datetime\n",
    "from datetime import date\n",
    "from itertools import repeat \n",
    "from progressbar import ProgressBar\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./data/road-train-pre.csv', error_bad_lines=False)\n",
    "data_test = pd.read_csv('./data/road-test-pre.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['event time:timestamp'] = pd.to_datetime(data_train['event time:timestamp'])\n",
    "data_train = data_train.sort_values(by=['case concept:name', 'event time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['event time:timestamp'] = pd.to_datetime(data_test['event time:timestamp'])\n",
    "data_test = data_test.sort_values(by=['case concept:name', 'event time:timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv(\"fixed.csv\")\n",
    "\n",
    "file = open('fixed.csv', 'r') \n",
    "log = dict()  # dictionary that contains all information for a case - key: case name; values: events, timestamps\n",
    "with open('fixed.csv', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        parts = line.split(',')\n",
    "        caseid = parts[2]\n",
    "\n",
    "        task = parts[3]\n",
    "        timestamp = parts[5]\n",
    "\n",
    "        if caseid not in log:\n",
    "            log[caseid] = [[],[]]\n",
    "\n",
    "        log[caseid][0].append(task)  # adding the events as a list into the dictionary\n",
    "        log[caseid][1].append(timestamp)  # adding the timestamps as a list into the dictionary\n",
    "        \n",
    "file.close()\n",
    "\n",
    "os.remove('fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in log.keys():  # updating the dictionary to contain also all next events \n",
    "    current = log[i][0]  # recording the cuurent case' events\n",
    "    \n",
    "    real_next = current[1:]  # next real events\n",
    "    real_next.append('New Case')  # adding a 'new case' as real next event for every last event\n",
    "    \n",
    "    log[i].append(real_next)  # adding the real next events to the log file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Repeating the same process from above on the test data.\n",
    "\n",
    "data_test.to_csv(\"fixed_test.csv\")\n",
    "\n",
    "file = open('fixed_test.csv', 'r')\n",
    "log_test = dict()\n",
    "with open('fixed_test.csv', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        parts = line.split(',')\n",
    "        caseid = parts[2]\n",
    "        \n",
    "        task = parts[3]\n",
    "        timestamp = parts[5]\n",
    "\n",
    "        if caseid not in log_test:\n",
    "            log_test[caseid] = [[],[]]\n",
    "\n",
    "        log_test[caseid][0].append(task)\n",
    "        log_test[caseid][1].append(timestamp)\n",
    "        \n",
    "file.close()\n",
    "\n",
    "os.remove('fixed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fixing a bug of cases that are in the test data but are incomplete due to the train-test split.\"\"\"\n",
    "\n",
    "bugs = []\n",
    "\n",
    "for i in log_test.keys():  #  recording the cases which have events cut because of the train - test split\n",
    "    if len(log_test[i][0]) == 1:\n",
    "        bugs.append(i)\n",
    "            \n",
    "for x in bugs:  # deleting the above mentioned events \n",
    "    del log_test[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in log_test.keys():\n",
    "    current = log_test[i][0]  # current case' events\n",
    "    \n",
    "    real_next = current[1:]  # next real events\n",
    "    real_next.append('New Case')  # adding a 'new case' as real next event for every last event\n",
    "    log_test[i].append(real_next) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mumbo Jumbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "\n",
    "tenabove = []\n",
    "for i in log.keys():\n",
    "    if len(log[i][0]) > m:\n",
    "        m = len(log[i][0])\n",
    "        \n",
    "    if len(log[i][0]) > 10:\n",
    "        tenabove.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenabove_test = []\n",
    "for i in log_test.keys():\n",
    "    if len(log_test[i][0]) > m:\n",
    "        m = len(log_test[i][0])\n",
    "        \n",
    "    if len(log_test[i][0]) > 10:\n",
    "        tenabove_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete = []\n",
    "for i in log_test.keys():\n",
    "    if len(log_test[i][0]) > m:\n",
    "        print(i)\n",
    "        print(log_test[i])\n",
    "        delete.append(i)\n",
    "        #m_t = len(log_test[i][0])\n",
    "        \n",
    "for i in delete:\n",
    "    print(i)\n",
    "    \n",
    "    test.drop(test.index[test['case concept:name'] == i], inplace = True)\n",
    "\n",
    "    del log_test[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Storing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  new dictionary that will contain for every position(key) the observed traces and next events for each trace(values)\n",
    "#  so case [A, B, C] would be saved as {0:[[A],[B]], 1: [[A,B], [C]], 2: [[A, B, C], [New Case]]} \n",
    "train_data = {} \n",
    "\n",
    "for i in log.keys():\n",
    "    count = 0\n",
    "    for x in log[i][0]:\n",
    "        case = log[i][0]\n",
    "        #ind = log[i][0].index(x)\n",
    "        \n",
    "        if count not in train_data:  # making the two lists in the dictionary\n",
    "            train_data[count] = [[],[]]  # list 1 is all for all traces of the position, list 2 is for all next events\n",
    "        \n",
    "        \n",
    "        train_data[count][0].append(case[:count+1])  # appending the trace\n",
    "        \n",
    "        if count < len(case)-1:\n",
    "            train_data[count][1].append(case[count+1])  # appending the next event of the trace\n",
    "            \n",
    "        elif count == len(case)-1:\n",
    "            train_data[count][1].append('New Case')\n",
    "            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  repeating the same process on the test data\n",
    "test_data = {} \n",
    "\n",
    "for i in log_test.keys():\n",
    "    count = 0\n",
    "    for x in log_test[i][0]:\n",
    "        case = log_test[i][0]\n",
    "        #ind = log_test[i][0].index(x)\n",
    "        \n",
    "        if count not in test_data:\n",
    "            test_data[count] = [[],[]]\n",
    "        \n",
    "        \n",
    "        test_data[count][0].append(case[:count+1])  # appending the trace\n",
    "        \n",
    "        if count < len(case)-1:\n",
    "            test_data[count][1].append(case[count+1])  # appending the next event of the trace\n",
    "            \n",
    "        elif count == len(case)-1:\n",
    "            test_data[count][1].append('New Case')\n",
    "            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all unique event names of all the data into integers\n",
    "\n",
    "cases = list(data_train['event concept:name'].unique()) + list(data_test['event concept:name'].unique())  # all events\n",
    "cases.append('New Case')  #  adding the 'New Case' because we predict next event is going to be new case\n",
    "cases = list(set(cases)) \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(cases)  # encoding all event names into integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "\n",
    "for i in pbar(train_data.keys()):  # the dictionaries from above are encoded into integers \n",
    "    \n",
    "    encoded = []\n",
    "    for trace in train_data[i][0]:  # encoding all strings of a trace, can be multiple if case lenght is more than 2\n",
    "        local_encoded = []\n",
    "        for event in trace:\n",
    "            local_encoded.append(int(le.transform([event])))  # transforming into integer\n",
    "        encoded.append(local_encoded)\n",
    "    \n",
    "    train_data[i][0] = np.array(encoded)  # making the list with integers into array so the tree can take it\n",
    "    \n",
    "    \n",
    "    encoded_next = []  # encoding all strings of next events for a trace, its always length 1 !\n",
    "    for g in train_data[i][1]:\n",
    "        encoded_next.append(int(le.transform([g])))  # transforming into integer\n",
    "                            \n",
    "                            \n",
    "    train_data[i][1] = np.array(encoded_next)  #  making the list with integers into array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating the procedure from above on the test data\n",
    "\n",
    "pbar = ProgressBar()\n",
    "\n",
    "for i in pbar(test_data.keys()):\n",
    "    \n",
    "    encoded = []\n",
    "    for trace in test_data[i][0]:\n",
    "        local_encoded = []\n",
    "        for event in trace:\n",
    "            local_encoded.append(int(le.transform([event])))\n",
    "        encoded.append(local_encoded)\n",
    "    \n",
    "    test_data[i][0] = np.array(encoded)\n",
    "    \n",
    "    \n",
    "    encoded_next = []\n",
    "    for g in test_data[i][1]:\n",
    "        encoded_next.append(int(le.transform([g])))\n",
    "                            \n",
    "                            \n",
    "    test_data[i][1] = np.array(encoded_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training decision tree for any given position (as long as the position is in the train data)\n",
    "\n",
    "def decision_tree(pos):\n",
    "\n",
    "    x_train= train_data[pos][0]\n",
    "    y_train= train_data[pos][1]\n",
    "\n",
    "    classifier = DecisionTreeClassifier()\n",
    "    classifier.fit(x_train, y_train)\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = {}  # dictionary to contain all decision trees given the position\n",
    "#  key - position, value - decision tree for that position\n",
    "\n",
    "for i in test_data.keys():\n",
    "    if i > len(train_data) - 1:\n",
    "        predictors[i] = decision_tree(len(train_data) - 1)\n",
    "        \n",
    "    else:\n",
    "        predictors[i] = decision_tree(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Adding predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "\n",
    "for i in pbar(log_test.keys()):  # adding an array with the encoding to the log_test dict. for every case in the test \n",
    "    current = log_test[i][0]\n",
    "    \n",
    "    \n",
    "    encoded = []  # list will contain all event names encoded into integers\n",
    "    for g in current:\n",
    "        encoded.append(int(le.transform([g])))\n",
    "    encoded = np.array(encoded)\n",
    "    log_test[i].append(encoded)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "for i in pbar(log_test.keys()):  # making predictions for every case in the log_test dict\n",
    "    \n",
    "    current_encoded = log_test[i][3]\n",
    "    predictions = []  # list that will contain all predictions for a given case\n",
    "    count = 0\n",
    "    \n",
    "    for x in current_encoded:\n",
    "        \n",
    "        \n",
    "        # the if-else is a checks whether the case length is more than any case length observed in the train data\n",
    "        if count >= len(train_data) - 1: # if its in the train data we call the appropriate decision tree\n",
    "            \n",
    "            \n",
    "            tree = predictors[len(train_data) - 1]  # calling the right tree given the position\n",
    "            p = current_encoded[:(len(train_data))]  # taking the trace\n",
    "            p = p.reshape(1, -1)\n",
    "            pred = tree.predict(p)  # making a prediction \n",
    "            pred_string = le.inverse_transform(pred)[0]  # transforming the prediction into a string\n",
    "            predictions.append(pred_string)  # appending the prediction as a string to the log_test data\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:  # if its not in the train data then we use the last observed decision tree from the train data\n",
    "        \n",
    "            tree = predictors[count]  # calling the right tree given the position\n",
    "            p = current_encoded[:count+1]  # taking the trace\n",
    "            p = p.reshape(1, -1)  # we need to do that, idk why\n",
    "            pred = tree.predict(p)  # making a prediction\n",
    "            pred_string = le.inverse_transform(pred)[0]  # transforming the prediction into a string\n",
    "            predictions.append(pred_string)  # appending the prediction as a string to the log_test data \n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "    log_test[i].append(predictions)  # adding all predictions to the log_test of the current case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making lists for every column we will have in the frame\n",
    "case_names = []\n",
    "event_names = []\n",
    "timestamp = []\n",
    "p_event = []\n",
    "current_real = []\n",
    "\n",
    "for i in log_test.keys():  # appending the right things to every list from the log_test file\n",
    "    for x in range(len(log_test[i][0])):\n",
    "        case_names.append(i)\n",
    "        event_names.append(log_test[i][0][x])\n",
    "        timestamp.append(log_test[i][1][x])\n",
    "        p_event.append(log_test[i][4][x])\n",
    "        current_real.append(log_test[i][2][x])\n",
    "\n",
    "# dictionary that will be used to make the frame\n",
    "frame_dict = {'Case_ID': case_names, 'Event_Name': event_names,\n",
    "              'TimeStamp': timestamp, 'Next_Event': current_real, 'Predicted_Event': p_event}\n",
    "predicted_df = pd.DataFrame.from_dict(frame_dict)  # making a frame \n",
    "\n",
    "event_real = np.array(predicted_df['Next_Event'])  # taking next event col. as an array\n",
    "event_pred = np.array(predicted_df['Predicted_Event'])  # taking the predictions as an array \n",
    "\n",
    "acc = accuracy_score(event_real, event_pred)  # calculates the accuracy based on the both arrays\n",
    "print('Accuracy for event prediction TEST SET: {}%'.format(round(acc, 2) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy per position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- Accuracy per position on train data\\n')\n",
    "for i in train_data.keys():\n",
    "    \n",
    "    x_train = train_data[i][0]\n",
    "    y_train = train_data[i][1]\n",
    "    \n",
    "    tree = predictors[i]\n",
    "    \n",
    "    x_test = train_data[i][0]\n",
    "    y_test = train_data[i][1]\n",
    "    \n",
    "    y_pred = tree.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print('-- Position:', i, '-- Acc:', '{}%'.format(round(acc, 2) * 100), '-- Cases:', len(x_test),\n",
    "          '-- Cases to train on:', len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- Accuracy per position on test data\\n')\n",
    "for i in test_data.keys():\n",
    "    \n",
    "    x_test = test_data[i][0]\n",
    "    y_test = test_data[i][1]\n",
    "    \n",
    "    if i <= len(train_data) - 1:\n",
    "\n",
    "        tree = predictors[i]\n",
    "\n",
    "        y_pred = tree.predict(x_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print('-- Position:', i, '-- Acc:', '{}%'.format(round(acc, 2) * 100), '-- Cases:', len(x_test),\n",
    "             '-- Cases to train on:', len(train_data[i][0]) )\n",
    "        \n",
    "    elif i > len(train_data) - 1:\n",
    "        \n",
    "        new_trace = [x[:len(train_data)] for x in x_test]\n",
    "        tree = predictors[len(train_data) - 1]\n",
    "        y_pred = tree.predict(new_trace)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print('-- Position:', i, '-- Acc:', '{}%'.format(round(acc, 2) * 100), '-- Cases:', len(x_test), \n",
    "             '-- Cases to train on: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression for timestamp prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_train\n",
    "test = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new useful columns for the model train\n",
    "train['position_event']=train.groupby('case concept:name').cumcount()\n",
    "train['position_event']=train['position_event']+1\n",
    "train['week_day']=train['event time:timestamp'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding all event names into integers\n",
    "cases = train['event concept:name'].unique().tolist()\n",
    "cases.insert(0, 'New Case')\n",
    "le_case = preprocessing.LabelEncoder()\n",
    "le_case.fit(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding lifecycle into integers\n",
    "life = train['event lifecycle:transition'].unique().tolist()\n",
    "le_life = preprocessing.LabelEncoder()\n",
    "le_life.fit(life)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data for model train\n",
    "#Event poistion\n",
    "x_train_position = np.array(train['position_event']).reshape(-1,1)[:]\n",
    "#Previous event\n",
    "x_train_prev = list(train['event concept:name'])\n",
    "x_train_prev= le_case.transform(x_train_prev)\n",
    "x_train_prev = np.array(x_train_prev).reshape(-1,1)[:]\n",
    "# Event\n",
    "x_train_event = list(train['event concept:name'])\n",
    "x_train_event.insert(len(train), 'New Case')\n",
    "x_train_event= le_case.transform(x_train_event)\n",
    "x_train_event = np.array(x_train_event).reshape(-1,1)[1:]\n",
    "#Day of the week previous event event\n",
    "x_train_week = list(train['week_day'])\n",
    "x_train_week = np.array(x_train_week).reshape(-1,1)[:]\n",
    "#Timestamp event\n",
    "train[['event time:timestamp']] = train[['event time:timestamp']].astype(str)\n",
    "x_train_date = list(train['event time:timestamp'])\n",
    "x_train_date.insert(len(train), None)\n",
    "x_train_date=np.array(x_train_date).reshape(-1,1)[1:]\n",
    "#Timestamp previous event\n",
    "x_train_date_prev = list(train['event time:timestamp'])\n",
    "x_train_date_prev=np.array(x_train_date_prev).reshape(-1,1)[:]\n",
    "#Event Lifecycle\n",
    "x_train_life = list(train['event lifecycle:transition'])\n",
    "x_train_life= le_life.transform(x_train_life)\n",
    "x_train_life = np.array(x_train_life).reshape(-1,1)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length case for train set\n",
    "cases = train.groupby(['case concept:name'])\n",
    "per_case = pd.DataFrame({'no of events':cases['eventID '].count()})\n",
    "lst_per_case = per_case[\"no of events\"].tolist()\n",
    "case_length = []\n",
    "for length in lst_per_case:\n",
    "    case_length.extend(repeat(length, length))\n",
    "x_train_length_case=np.array(case_length).reshape(-1,1)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine features for the model train\n",
    "x_train_new = np.concatenate((x_train_position,x_train_prev, x_train_event, x_train_week, x_train_date,\n",
    "                              x_train_date_prev, x_train_length_case, x_train_life), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features to new dataframe train\n",
    "df_train = pd.DataFrame(data=x_train_new, columns=['position_event', 'prev_event', 'event', 'week_day_prev', 'date', 'date_prev', 'case_length', 'lifecycle'])\n",
    "df_train.loc[df_train['position_event'] == df_train['case_length'], 'event'] = 5\n",
    "df_train[['date','date_prev']] = df_train[['date','date_prev']].apply(pd.to_datetime)\n",
    "df_train.loc[df_train['event'] == 5, 'date'] = None\n",
    "df_train['in_between'] = (df_train['date'] - df_train['date_prev']).dt.days\n",
    "df_train.loc[df_train['event'] == 5, 'in_between'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing dummies train\n",
    "df_train=pd.get_dummies(df_train, columns=['event', 'prev_event', 'week_day_prev', 'position_event', 'lifecycle'])\n",
    "df_train = df_train.drop(['date', 'date_prev'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new useful columns for the model test\n",
    "test['position_event']=test.groupby('case concept:name').cumcount()\n",
    "test['position_event']=test['position_event']+1\n",
    "test['week_day']=test['event time:timestamp'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_events=predicted_df['Predicted_Event'][:].tolist()\n",
    "test['pred_event']=predicted_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data for model test\n",
    "#Event poistion\n",
    "x_test_position = np.array(test['position_event']).reshape(-1,1)[:]\n",
    "#Previous event\n",
    "x_test_prev = test['event concept:name'].tolist()\n",
    "x_test_prev = le_case.transform(x_test_prev)\n",
    "x_test_prev = np.array(x_test_prev).reshape(-1,1)[:]\n",
    "#Predicted Event\n",
    "x_test_event = test['pred_event'].tolist()\n",
    "x_test_event= le_case.transform(x_test_event)\n",
    "x_test_event = np.array(x_test_event).reshape(-1,1)[:]\n",
    "#Day of the week previous event\n",
    "x_test_week = test['week_day'].tolist()\n",
    "x_test_week = np.array(x_test_week).reshape(-1,1)[:]\n",
    "#Timestamp event\n",
    "test[['event time:timestamp']] = test[['event time:timestamp']].astype(str)\n",
    "x_test_date = list(test['event time:timestamp'])\n",
    "x_test_date.insert(len(test), None)\n",
    "x_test_date=np.array(x_test_date).reshape(-1,1)[1:]\n",
    "#Timestamp previous event\n",
    "x_test_date_prev = list(test['event time:timestamp'])\n",
    "x_test_date_prev=np.array(x_test_date_prev).reshape(-1,1)[:]\n",
    "#Event Lifecycle\n",
    "x_test_life = test['event lifecycle:transition'].tolist()\n",
    "x_test_life= le_life.transform(x_test_life)\n",
    "x_test_life = np.array(x_test_life).reshape(-1,1)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length case for test set\n",
    "test_cases = test.groupby(['case concept:name'])\n",
    "per_case_test = pd.DataFrame({'no of events':test_cases['eventID '].count()})\n",
    "lst_per_case_test = per_case_test[\"no of events\"].tolist()\n",
    "case_length_test = []\n",
    "for length in lst_per_case_test:\n",
    "    case_length_test.extend(repeat(length, length))\n",
    "x_test_length_case=np.array(case_length_test).reshape(-1,1)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine features for the model test\n",
    "x_test_new = np.concatenate((x_test_position ,x_test_prev, x_test_event, x_test_week, x_test_date,\n",
    "                             x_test_date_prev, x_test_length_case, x_test_life), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features to new dataframe test\n",
    "df_test = pd.DataFrame(data=x_test_new, columns=['position_event', 'prev_event', 'event', 'week_day_prev',\n",
    "                                                 'date', 'date_prev', 'case_length', 'lifecycle'])\n",
    "df_test.loc[df_test['position_event'] == df_test['case_length'], 'date'] = None\n",
    "df_test[['date','date_prev']] = df_test[['date','date_prev']].apply(pd.to_datetime)\n",
    "df_test['in_between'] = (df_test['date'] - df_test['date_prev']).dt.days\n",
    "df_test.loc[df_test['position_event'] == df_test['case_length'], 'in_between'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove cases with more events than the cases in the train set\n",
    "df_test=df_test[df_test['case_length']<=max(df_train['case_length'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dumies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing dummies test\n",
    "df_test=pd.get_dummies(df_test, columns=['event', 'prev_event', 'week_day_prev', 'position_event', 'lifecycle'])\n",
    "df_test = df_test.drop(['date', 'date_prev'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_test=df_test.columns\n",
    "features=set(col_train).intersection(col_test)\n",
    "features.discard('in_between')\n",
    "X_train = df_train[features] # Features\n",
    "y_train = df_train['in_between'] # Target variable\n",
    "X_test = df_test[features] # Features\n",
    "y_test = df_test['in_between'] # Target variable\n",
    "\n",
    "#Training the algorithm\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "df_predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root Mean Squared Error of the model\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
