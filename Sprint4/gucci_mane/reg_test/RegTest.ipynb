{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:26:29.283066Z",
     "start_time": "2020-04-02T15:26:27.920278Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "from itertools import tee, repeat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from progressbar import ProgressBar\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:26:29.938910Z",
     "start_time": "2020-04-02T15:26:29.926942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Asus S\\\\sakce_git\\\\process\\\\Sprint4\\\\gucci_mane'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:26:31.501043Z",
     "start_time": "2020-04-02T15:26:31.497056Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = './data/road-train.csv'\n",
    "test_path = './data/road-test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:26:33.037154Z",
     "start_time": "2020-04-02T15:26:33.025181Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "'''For accuracy and rmse - do it for all three new columns'''\n",
    "def pre_data(train_path: str, test_path: str) -> object:\n",
    "\n",
    "    if train_path == './data/2018-train.csv':\n",
    "        df2_test = pd.read_csv(test_path, encoding = \"ISO-8859-1\",\n",
    "        error_bad_lines = False, dtype = {'event org:resource': str}, engine='python')\n",
    "        df2_train = pd.read_csv(train_path, encoding = \"ISO-8859-1\",\n",
    "        error_bad_lines = False, dtype = {'event org:resource': str}, engine='python')\n",
    "    else:\n",
    "        df2_test = pd.read_csv(train_path)\n",
    "        df2_train = pd.read_csv(test_path)\n",
    "\n",
    "    # Convert from string to datetime\n",
    "    df2_train['event time:timestamp'] = pd.to_datetime(df2_train['event time:timestamp'])\n",
    "    df2_test['event time:timestamp'] = pd.to_datetime(df2_test['event time:timestamp'])\n",
    "\n",
    "    # Concatenate the datasets\n",
    "    df2 = pd.concat([df2_train, df2_test])\n",
    "\n",
    "    # Sort the values and reset the index accordingly\n",
    "    df2 = df2.sort_values(by=['event time:timestamp'])\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    # Create dataframe with first events of each case in chronological order\n",
    "    df_cases_first_event = df2.drop_duplicates(subset='case concept:name', keep='first')\n",
    "    df_cases_first_event = df_cases_first_event.reset_index(drop=True)\n",
    "\n",
    "    # Split the cases in 80-20\n",
    "    index80 = int(len(df_cases_first_event) * 80 / 100 - 1)\n",
    "    df_cases_train = df_cases_first_event.loc[:index80]\n",
    "    df_cases_test = df_cases_first_event.loc[index80 + 1:]\n",
    "\n",
    "    # Training and test set complete\n",
    "    df_train_new = df2.loc[df2['case concept:name'].isin(list(df_cases_train['case concept:name']))]\n",
    "    df_test_new = df2.loc[df2['case concept:name'].isin(list(df_cases_test['case concept:name']))]\n",
    "\n",
    "    # Setting the 'now' (as the last event of the last case in the training set)\n",
    "    # now = df_train_new[df_train_new['case concept:name']==df_cases_train.iloc[-1]\\\n",
    "    #    ['case concept:name']].iloc[-1]['event time:timestamp']\n",
    "    if 'case LoanGoal' in df2_train:\n",
    "        now = pd.Timestamp('2016-12-08')\n",
    "    elif len(df2_train.columns) == 5:\n",
    "        now = pd.Timestamp('2012-12-12')\n",
    "    elif 'case AMOUNT_REQ' in df2_train:\n",
    "        now = pd.Timestamp('2012-10-02')\n",
    "    else:\n",
    "        now = pd.Timestamp('2017-11-12')\n",
    "\n",
    "    # Training and test set before 'now'\n",
    "    df_train_new_now = df_train_new.loc[(df_train_new['event time:timestamp'] <= now)]\n",
    "    df_test_new_now = df_test_new.loc[(df_test_new['event time:timestamp'] <= now)]\n",
    "\n",
    "    return df_train_new_now, df_test_new_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:26:38.076385Z",
     "start_time": "2020-04-02T15:26:36.314681Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train, data_test = pre_data(train_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree to output file for LR\n",
    "\n",
    "To skip running the whole DT, run the cell that reads a different csv file(which is exactly what the decision tree would output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:22:04.067503Z",
     "start_time": "2020-04-02T15:18:46.038686Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23% |################                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      " 76% |######################################################                  |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      " 20% |##############                                                          |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "  3% |##                                                                      |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "  1% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "def de_tree(data_train, data_test):\n",
    "    \n",
    "    data_train['event time:timestamp'] = pd.to_datetime(data_train['event time:timestamp'])\n",
    "    data_train = data_train.sort_values(by=['case concept:name', 'event time:timestamp'])\n",
    "\n",
    "    data_test['event time:timestamp'] = pd.to_datetime(data_test['event time:timestamp'])\n",
    "    data_test = data_test.sort_values(by=['case concept:name', 'event time:timestamp'])\n",
    "\n",
    "    # LOAN DATA\n",
    "    if 'case LoanGoal' in data_train.columns:\n",
    "        data_train['case LoanGoal'] = data_train['case LoanGoal'].apply(\n",
    "            lambda x: 'Other - see explanation' if x == 'Other, see explanation' else x)\n",
    "\n",
    "        data_test['case LoanGoal'] = data_test['case LoanGoal'].apply(\n",
    "            lambda x: 'Other - see explanation' if x == 'Other, see explanation' else x)\n",
    "\n",
    "    pt1 = data_train.columns.get_loc('case concept:name') + 1\n",
    "    pt2 = data_train.columns.get_loc('event concept:name') + 1\n",
    "    pt3 = data_train.columns.get_loc('event time:timestamp') + 1\n",
    "\n",
    "    # DANGER\n",
    "    cases = list(data_train['event concept:name'].unique()) + list(data_test['event concept:name'].unique())\n",
    "    cases.append('New Case')\n",
    "    cases = list(set(cases))\n",
    "\n",
    "    # 1. Train Data\n",
    "\n",
    "    # DANGER\n",
    "    data_train.to_csv(\"./build/fixed.csv\")\n",
    "\n",
    "    log = dict()\n",
    "    with open('./build/fixed.csv', 'r') as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            parts = line.split(',')\n",
    "            caseid = parts[pt1]\n",
    "\n",
    "            task = parts[pt2]\n",
    "            # added two lines\n",
    "            # first one to have numeric labels for weekdays, +len cases to prevent same labeling\n",
    "            # last one is only for the predicted_df in the end\n",
    "            timestamp = pd.to_datetime(parts[pt3]).weekday() + len(cases)\n",
    "            timestamp_full = parts[pt3]\n",
    "            if caseid not in log:\n",
    "                log[caseid] = [[], [], []]\n",
    "\n",
    "            log[caseid][0].append(task)\n",
    "\n",
    "            # append the timestamps.\n",
    "            log[caseid][1].append(timestamp_full)\n",
    "            log[caseid][2].append(timestamp)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    os.remove('./build/fixed.csv')\n",
    "\n",
    "    for i in log.keys():  # updating the dictionary to contain also all next events\n",
    "        current = log[i][0]  # recording the cuurent case' events\n",
    "\n",
    "        real_next = current[1:]  # next real events\n",
    "        real_next.append('New Case')  # adding a 'new case' as real next event for every last event\n",
    "\n",
    "        log[i].append(real_next)  # adding the real next events to the log file\n",
    "\n",
    "    # 2. Test Data\n",
    "\n",
    "    # DANGER\n",
    "    data_test.to_csv(\"./build/fixed_test.csv\")\n",
    "\n",
    "    log_test = dict()\n",
    "    with open('./build/fixed_test.csv', 'r') as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            parts = line.split(',')\n",
    "            caseid = parts[pt1]\n",
    "            # same as training\n",
    "            task = parts[pt2]\n",
    "            timestamp = pd.to_datetime(parts[pt3]).weekday() + len(cases)\n",
    "            timestamp_full = parts[pt3]\n",
    "\n",
    "            if caseid not in log_test:\n",
    "                log_test[caseid] = [[], [], []]\n",
    "\n",
    "            log_test[caseid][0].append(task)\n",
    "            log_test[caseid][1].append(timestamp_full)\n",
    "            log_test[caseid][2].append(timestamp)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    os.remove('./build/fixed_test.csv')\n",
    "\n",
    "    \"\"\"Fixing a bug of cases that are in the test data but are incomplete due to the train-test split.\"\"\"\n",
    "\n",
    "    bugs = []\n",
    "\n",
    "    for i in log_test.keys():  # recording the cases which have events cut because of the train - test split\n",
    "        if len(log_test[i][0]) == 1:\n",
    "            bugs.append(i)\n",
    "\n",
    "    for x in bugs:  # deleting the above mentioned events\n",
    "        del log_test[x]\n",
    "        data_test.drop(data_test.index[data_test['case concept:name'] == x], inplace=True)\n",
    "\n",
    "    for i in log_test.keys():\n",
    "        current = log_test[i][0]  # current case' events\n",
    "\n",
    "        real_next = current[1:]  # next real events\n",
    "        real_next.append('New Case')  # adding a 'new case' as real next event for every last event\n",
    "        log_test[i].append(real_next)\n",
    "\n",
    "        # 3. Storing the data\n",
    "\n",
    "    #  new dictionary that will contain for every position(key) the observed traces and next events for each trace(values)\n",
    "    #  so case [A, B, C] would be saved as {0:[[A],[B]], 1: [[A,B], [C]], 2: [[A, B, C], [New Case]]}\n",
    "    train_data = {}\n",
    "    pbar = ProgressBar()\n",
    "    print('[12/18]')\n",
    "\n",
    "    for i in pbar(log.keys()):\n",
    "        count = 0\n",
    "        for x in log[i][0]:\n",
    "            case = log[i][0]\n",
    "            time = log[i][2]  # DANGER\n",
    "\n",
    "            if count not in train_data:  # making the two lists in the dictionary\n",
    "                train_data[count] = [[], [],\n",
    "                                     []]  # list 1 is all for all traces of the position, list 2 is for all next events\n",
    "            # DANGER EXTRA LIST\n",
    "            train_data[count][0].append(case[:count + 1])  # appending the trace\n",
    "            train_data[count][2].append(time[count])  # DANGER\n",
    "            if count < len(case) - 1:\n",
    "                train_data[count][1].append(case[count + 1])  # appending the next event of the trace\n",
    "\n",
    "            elif count == len(case) - 1:\n",
    "                train_data[count][1].append('New Case')\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    #  repeating the same process on the test data\n",
    "    test_data = {}\n",
    "    pbar = ProgressBar()\n",
    "    print('[13/18]')\n",
    "    for i in pbar(log_test.keys()):\n",
    "        count = 0\n",
    "        for x in log_test[i][0]:\n",
    "            case = log_test[i][0]\n",
    "            time = log_test[i][2]  # DANGER\n",
    "\n",
    "            if count not in test_data:\n",
    "                test_data[count] = [[], [], []]\n",
    "\n",
    "            test_data[count][0].append(case[:count + 1])  # appending the trace\n",
    "            test_data[count][2].append(time[count])  # DANGER\n",
    "\n",
    "            if count < len(case) - 1:\n",
    "                test_data[count][1].append(case[count + 1])  # appending the next event of the trace\n",
    "\n",
    "            elif count == len(case) - 1:\n",
    "                test_data[count][1].append('New Case')\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    # 4. Encoding\n",
    "\n",
    "    # encoding all unique event names of all the data into integers\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(cases)  # encoding all event names into integers\n",
    "\n",
    "    ### 4.1 TRAIN\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    print('[14/18]')\n",
    "\n",
    "    for i in pbar(train_data.keys()):  # the dictionaries from above are encoded into integers\n",
    "\n",
    "        encoded = []\n",
    "        for trace in train_data[i][0]:  # encoding all strings of a trace, can be multiple if case lenght is more than 2\n",
    "            local_encoded = []\n",
    "            for event in trace:\n",
    "                local_encoded.append(int(le.transform([event])))  # transforming into integer\n",
    "            encoded.append(local_encoded)\n",
    "\n",
    "        train_data[i][0] = np.array(encoded)  # making the list with integers into array so the tree can take it\n",
    "\n",
    "        encoded_next = []  # encoding all strings of next events for a trace, its always length 1 !\n",
    "        for g in train_data[i][1]:\n",
    "            encoded_next.append(int(le.transform([g])))  # transforming into integer\n",
    "\n",
    "        train_data[i][1] = np.array(encoded_next)  # making the list with integers into array\n",
    "\n",
    "    ### 4.2 TEST\n",
    "\n",
    "    # repeating the procedure from above on the test data\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    print('[15/18]')\n",
    "\n",
    "    for i in pbar(test_data.keys()):\n",
    "\n",
    "        encoded = []\n",
    "        for trace in test_data[i][0]:\n",
    "            local_encoded = []\n",
    "            for event in trace:\n",
    "                local_encoded.append(int(le.transform([event])))\n",
    "            encoded.append(local_encoded)\n",
    "\n",
    "        test_data[i][0] = np.array(encoded)\n",
    "\n",
    "        encoded_next = []\n",
    "        for g in test_data[i][1]:\n",
    "            encoded_next.append(int(le.transform([g])))\n",
    "\n",
    "        test_data[i][1] = np.array(encoded_next)\n",
    "\n",
    "    # 5. Training the decision tree\n",
    "\n",
    "    # Function for training decision tree for any given position (as long as the position is in the train data)\n",
    "\n",
    "    # DANGER\n",
    "\n",
    "    def decision_tree(pos):\n",
    "        x_train = train_data[pos][0]\n",
    "        # combine full trace data with weekday\n",
    "        x_week = np.array(train_data[pos][2]).reshape(-1, 1)\n",
    "        x_new = np.concatenate((x_train, x_week), axis=1)\n",
    "        y_train = train_data[pos][1]\n",
    "\n",
    "        classifier = DecisionTreeClassifier()\n",
    "        classifier.fit(x_new, y_train)\n",
    "\n",
    "        return classifier\n",
    "\n",
    "    predictors = {}  # dictionary to contain all decision trees given the position\n",
    "    #  key - position, value - decision tree for that position\n",
    "    pbar = ProgressBar()\n",
    "    print('[16/18]')\n",
    "\n",
    "    for i in pbar(test_data.keys()):\n",
    "        if i > len(train_data) - 1:\n",
    "            predictors[i] = decision_tree(len(train_data) - 1)\n",
    "\n",
    "        else:\n",
    "            predictors[i] = decision_tree(i)\n",
    "\n",
    "    # 6. Adding predictions\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    print('[17/18]')\n",
    "\n",
    "    for i in pbar(\n",
    "            log_test.keys()):  # adding an array with the encoding to the log_test dict. for every case in the test\n",
    "        current = log_test[i][0]\n",
    "\n",
    "        encoded = []  # list will contain all event names encoded into integers\n",
    "        for g in current:\n",
    "            encoded.append(int(le.transform([g])))\n",
    "        encoded = np.array(encoded)\n",
    "        log_test[i].append(encoded)\n",
    "\n",
    "    def update_tree(case):\n",
    "\n",
    "        case = case.tolist()\n",
    "\n",
    "        count = 0\n",
    "        for x in case:  # case is an array\n",
    "            if count not in train_data:  # making the two lists in the dictionary\n",
    "                # list 1 is all for all traces of the position, list 2 is for all next events\n",
    "                train_data[count] = [np.array([]), np.array([])]\n",
    "            train_data[count][0] = train_data[count][0].tolist()\n",
    "            train_data[count][1] = train_data[count][1].tolist()\n",
    "\n",
    "            train_data[count][0].append(case[:count + 1])  # appending the trace\n",
    "\n",
    "            if count < len(case) - 1:\n",
    "                train_data[count][1].append(case[count + 1])  # appending the next event of the trace\n",
    "\n",
    "            elif count == len(case) - 1:\n",
    "                train_data[count][1].append(int(le.transform(['New Case'])))\n",
    "\n",
    "            train_data[count][0] = np.array(train_data[count][0])\n",
    "            train_data[count][1] = np.array(train_data[count][1])\n",
    "\n",
    "            predictors[count] = decision_tree(count)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    print('[18/18]')\n",
    "\n",
    "    for i in pbar(log_test.keys()):  # making predictions for every case in the log_test dict\n",
    "\n",
    "        current_encoded = log_test[i][4]  # DANGER\n",
    "        times = log_test[i][1]\n",
    "        weeks = log_test[i][2]\n",
    "        predictions = []  # list that will contain all predictions for a given case\n",
    "        count = 0\n",
    "\n",
    "        for x in current_encoded:\n",
    "\n",
    "            # the if-else is a checks whether the case length is more than any case length observed in the train data\n",
    "            if count >= len(train_data) - 1:  # if its in the train data we call the appropriate decision tree\n",
    "\n",
    "                tree = predictors[len(train_data) - 1]\n",
    "                p_trace = current_encoded[:(len(train_data))].reshape(-1, len(train_data))\n",
    "                # Create new array with full trace and weekdays, same as with train\n",
    "                p_weeks = np.array(weeks[len(train_data) - 1]).reshape(-1, 1)\n",
    "                p_new = np.concatenate((p_trace, p_weeks), axis=1)\n",
    "                pred = tree.predict(p_new)\n",
    "                pred_string = le.inverse_transform(pred)[0]\n",
    "                predictions.append(pred_string)\n",
    "\n",
    "\n",
    "\n",
    "            else:  # if its not in the train data then we use the last observed decision tree from the train data\n",
    "\n",
    "                tree = predictors[count]\n",
    "                p_trace = current_encoded[:count + 1].reshape(-1, count + 1)\n",
    "                # same as above\n",
    "                p_weeks = np.array(weeks[count]).reshape(-1, 1)\n",
    "                p_new = np.concatenate((p_trace, p_weeks), axis=1)\n",
    "                pred = tree.predict(p_new)\n",
    "                pred_string = le.inverse_transform(pred)[0]\n",
    "                predictions.append(pred_string)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        log_test[i].append(predictions)  # adding all predictions to the log_test of the current case\n",
    "\n",
    "        # UNCOMMENT THE LINE BELOW FOR ONLINE TRAINING\n",
    "\n",
    "        # update_tree(current_encoded)\n",
    "\n",
    "    # 7. Evaluation\n",
    "\n",
    "    # making lists for every column we will have in the frame\n",
    "    case_names = []\n",
    "    event_names = []\n",
    "    timestamp = []\n",
    "    p_event = []\n",
    "    current_real = []\n",
    "\n",
    "    for i in log_test.keys():  # appending the right things to every list from the log_test file\n",
    "        for x in range(len(log_test[i][0])):\n",
    "            case_names.append(i)\n",
    "            event_names.append(log_test[i][0][x])\n",
    "            timestamp.append(log_test[i][1][x])\n",
    "            p_event.append(log_test[i][-1][x])\n",
    "            current_real.append(log_test[i][3][x])\n",
    "\n",
    "    frame_dict = {'Case_ID': case_names, 'Event_Name': event_names,\n",
    "                  'TimeStamp': timestamp, 'Next_Event': current_real, 'DTree_Event': p_event}\n",
    "    \n",
    "    predicted_df = pd.DataFrame.from_dict(frame_dict)\n",
    "    \n",
    "    return predicted_df, le, data_train, data_test\n",
    "\n",
    "predicted_df, le, data_train, data_test = de_tree(data_train, data_test)\n",
    "\n",
    "os.chdir('reg_test')\n",
    "\n",
    "np.save('classes.npy', le.classes_)\n",
    "\n",
    "data_train.to_csv('./data_train.csv', index = False)\n",
    "data_test.to_csv('./data_test.csv', index = False)\n",
    "\n",
    "predicted_df.to_csv('./road_tree_csv.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:26:59.956939Z",
     "start_time": "2020-04-02T15:26:59.330449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this, if previous is not run.\n",
    "os.chdir('reg_test')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = np.load('classes.npy')\n",
    "\n",
    "data_train = pd.read_csv('./data_train.csv')\n",
    "data_test = pd.read_csv('./data_test.csv')\n",
    "predicted_df = pd.read_csv('./road_tree_csv.csv')\n",
    "\n",
    "data_train['event time:timestamp'] = pd.to_datetime(data_train['event time:timestamp'])\n",
    "data_test['event time:timestamp'] = pd.to_datetime(data_test['event time:timestamp'])\n",
    "predicted_df['TimeStamp'] = pd.to_datetime(predicted_df['TimeStamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-02T15:27:02.072Z"
    }
   },
   "outputs": [],
   "source": [
    "### We can wrap it in a function to run all at once, or keep outside to run cells individually\n",
    "\n",
    "#def lin_reg(data_train, data_test, predicted_df):\n",
    "#print(\"Timestamp Linear Regression Prediction...\")\n",
    "\n",
    "train = data_train.copy()\n",
    "test = data_test.copy()\n",
    "\n",
    "# Add new useful columns for the model train\n",
    "train['position_event'] = train.groupby('case concept:name').cumcount()\n",
    "train['position_event'] = train['position_event'] + 1\n",
    "train['week_day'] = train['event time:timestamp'].dt.dayofweek\n",
    "\n",
    "# Encoding all event names into integers\n",
    "cases = train['event concept:name'].unique().tolist()\n",
    "cases.insert(0, 'New Case')\n",
    "le_case = preprocessing.LabelEncoder()\n",
    "le_case.fit(cases)\n",
    "\n",
    "# Encoding lifecycle into integers\n",
    "life = train['event lifecycle:transition'].unique().tolist()\n",
    "le_life = preprocessing.LabelEncoder()\n",
    "le_life.fit(life)\n",
    "\n",
    "# Preprocess data for model train\n",
    "# Event poistion\n",
    "x_train_position = np.array(train['position_event']).reshape(-1, 1)[:]\n",
    "# Previous event\n",
    "x_train_prev = list(train['event concept:name'])\n",
    "x_train_prev = le_case.transform(x_train_prev)\n",
    "x_train_prev = np.array(x_train_prev).reshape(-1, 1)[:]\n",
    "# Event\n",
    "x_train_event = list(train['event concept:name'])\n",
    "x_train_event.insert(len(train), 'New Case')\n",
    "x_train_event = le_case.transform(x_train_event)\n",
    "x_train_event = np.array(x_train_event).reshape(-1, 1)[1:]\n",
    "# Day of the week previous event event\n",
    "x_train_week = list(train['week_day'])\n",
    "x_train_week = np.array(x_train_week).reshape(-1, 1)[:]\n",
    "# Timestamp event\n",
    "train[['event time:timestamp']] = train[['event time:timestamp']].astype(str)\n",
    "x_train_date = list(train['event time:timestamp'])\n",
    "x_train_date.insert(len(train), None)\n",
    "x_train_date = np.array(x_train_date).reshape(-1, 1)[1:]\n",
    "# Timestamp previous event\n",
    "x_train_date_prev = list(train['event time:timestamp'])\n",
    "x_train_date_prev = np.array(x_train_date_prev).reshape(-1, 1)[:]\n",
    "# Event Lifecycle\n",
    "x_train_life = list(train['event lifecycle:transition'])\n",
    "x_train_life = le_life.transform(x_train_life)\n",
    "x_train_life = np.array(x_train_life).reshape(-1, 1)[:]\n",
    "\n",
    "\n",
    "# Length case for train set\n",
    "cases = train.groupby(['case concept:name'])\n",
    "per_case = pd.DataFrame({'no of events': cases['eventID '].count()})\n",
    "lst_per_case = per_case[\"no of events\"].tolist()\n",
    "case_length = []\n",
    "for length in lst_per_case:\n",
    "    case_length.extend(repeat(length, length))\n",
    "x_train_length_case = np.array(case_length).reshape(-1, 1)[:]\n",
    "\n",
    "\n",
    "# Combine features for the model train\n",
    "x_train_new = np.concatenate((x_train_position, x_train_prev, x_train_event, x_train_week, x_train_date,\n",
    "                              x_train_date_prev, x_train_length_case, x_train_life), axis=1)\n",
    "\n",
    "# Add features to new dataframe train\n",
    "df_train = pd.DataFrame(data=x_train_new,\n",
    "                        columns=['position_event', 'prev_event', 'event', 'week_day_prev', 'date', 'date_prev',\n",
    "                                 'case_length', 'lifecycle'])\n",
    "df_train.loc[df_train['position_event'] == df_train['case_length'], 'event'] = int(le.transform(['New Case']))\n",
    "df_train[['date', 'date_prev']] = df_train[['date', 'date_prev']].apply(pd.to_datetime)\n",
    "df_train.loc[df_train['event'] == int(le.transform(['New Case'])), 'date'] = None\n",
    "df_train['in_between'] = (df_train['date'] - df_train['date_prev']).dt.total_seconds()\n",
    "df_train.loc[df_train['event'] == int(le.transform(['New Case'])), 'in_between'] = 0\n",
    "df_train[['case_length']] = df_train[['case_length']].astype(int)\n",
    "\n",
    "# Train Dummies\n",
    "\n",
    "# Implementing dummies train\n",
    "df_train = pd.get_dummies(df_train, columns=['event', 'prev_event', 'week_day_prev', 'position_event', 'lifecycle'])\n",
    "df_train = df_train.drop(['date', 'date_prev'], 1)\n",
    "\n",
    "# Test Data Preprocessing\n",
    "\n",
    "# Add new useful columns for the model test\n",
    "test['position_event'] = test.groupby('case concept:name').cumcount()\n",
    "test['position_event'] = test['position_event'] + 1\n",
    "test['week_day'] = test['event time:timestamp'].dt.dayofweek\n",
    "predicted_events = predicted_df['DTree_Event'][:].tolist()\n",
    "test['pred_event'] = predicted_events\n",
    "\n",
    "# Preprocess data for model test\n",
    "# Event poistion\n",
    "x_test_position = np.array(test['position_event']).reshape(-1, 1)[:]\n",
    "# Previous event\n",
    "x_test_prev = test['event concept:name'].tolist()\n",
    "x_test_prev = le_case.transform(x_test_prev)\n",
    "x_test_prev = np.array(x_test_prev).reshape(-1, 1)[:]\n",
    "# Predicted Event\n",
    "x_test_event = test['pred_event'].tolist()\n",
    "x_test_event = le_case.transform(x_test_event)\n",
    "x_test_event = np.array(x_test_event).reshape(-1, 1)[:]\n",
    "# Day of the week previous event\n",
    "x_test_week = test['week_day'].tolist()\n",
    "x_test_week = np.array(x_test_week).reshape(-1, 1)[:]\n",
    "# Timestamp event\n",
    "test[['event time:timestamp']] = test[['event time:timestamp']].astype(str)\n",
    "x_test_date = list(test['event time:timestamp'])\n",
    "x_test_date.insert(len(test), None)\n",
    "x_test_date = np.array(x_test_date).reshape(-1, 1)[1:]\n",
    "# Timestamp previous event\n",
    "x_test_date_prev = list(test['event time:timestamp'])\n",
    "x_test_date_prev = np.array(x_test_date_prev).reshape(-1, 1)[:]\n",
    "# Event Lifecycle\n",
    "x_test_life = test['event lifecycle:transition'].tolist()\n",
    "x_test_life = le_life.transform(x_test_life)\n",
    "x_test_life = np.array(x_test_life).reshape(-1, 1)[:]\n",
    "\n",
    "\n",
    "# Length case for test set\n",
    "test_cases = test.groupby(['case concept:name'])\n",
    "per_case_test = pd.DataFrame({'no of events': test_cases['eventID '].count()})\n",
    "lst_per_case_test = per_case_test[\"no of events\"].tolist()\n",
    "case_length_test = []\n",
    "for length in lst_per_case_test:\n",
    "    case_length_test.extend(repeat(length, length))\n",
    "x_test_length_case = np.array(case_length_test).reshape(-1, 1)[:]\n",
    "\n",
    "# Combine features for the model test\n",
    "x_test_new = np.concatenate((x_test_position, x_test_prev, x_test_event, x_test_week, x_test_date, x_test_date_prev,\n",
    "                             x_test_length_case, x_test_life), axis=1)\n",
    "\n",
    "# Add features to new dataframe test\n",
    "df_test = pd.DataFrame(data=x_test_new,\n",
    "                       columns=['position_event', 'prev_event', 'event', 'week_day_prev', 'date', 'date_prev',\n",
    "                                'case_length', 'lifecycle'])\n",
    "df_test.loc[df_test['position_event'] == df_test['case_length'], 'date'] = None\n",
    "df_test[['date', 'date_prev']] = df_test[['date', 'date_prev']].apply(pd.to_datetime)\n",
    "df_test['in_between'] = (df_test['date'] - df_test['date_prev']).dt.total_seconds()\n",
    "df_test.loc[df_test['position_event'] == df_test['case_length'], 'in_between'] = 0\n",
    "df_test[['case_length']] = df_test[['case_length']].astype(int)\n",
    "\n",
    "# Remove cases with more events than the cases in the train set\n",
    "df_test = df_test[df_test['case_length'] <= max(df_train['case_length'])]\n",
    "\n",
    "# Test Dummies\n",
    "\n",
    "# Implementing dummies test\n",
    "df_test_d = pd.get_dummies(df_test, columns=['event', 'prev_event', 'week_day_prev', 'position_event', 'lifecycle'])\n",
    "df_test_d = df_test_d.drop(['date', 'date_prev'], 1)\n",
    "\n",
    "# Feature selection and model training\n",
    "\n",
    "col_train = df_train.columns\n",
    "col_test = df_test_d.columns\n",
    "features = set(col_train).intersection(col_test)\n",
    "features.discard('in_between')\n",
    "X_train = df_train[features]  # Features\n",
    "y_train = df_train['in_between']  # Target variable\n",
    "X_test = df_test_d[features]  # Features\n",
    "y_test = df_test_d['in_between']  # Target variable\n",
    "\n",
    "# Training the algorithm\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "# Workaround to get rid of negative values\n",
    "# All 'New Case' to 0 and others to absolute value: RMSE 166.7941 (days)\n",
    "y_pred = regressor.predict(X_test)\n",
    "df_predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df_pred_fin = pd.concat([df_test, df_predictions], axis=1)\n",
    "df_pred_fin.loc[df_pred_fin['event'] == int(le.transform(['New Case'])), 'Predicted'] = 0\n",
    "y_pred = df_pred_fin['Predicted']\n",
    "y_pred = abs(y_pred)\n",
    "df_predictions = pd.DataFrame({'DTree_Actual_TimeDiff': y_test, 'DTree_TimeDiff': y_pred})\n",
    "\n",
    "predicted_df = pd.concat([predicted_df, df_predictions], axis=1)\n",
    "\n",
    "predicted_df = predicted_df[predicted_df['DTree_Actual_TimeDiff'].notna()]\n",
    "predicted_df = predicted_df[predicted_df['DTree_TimeDiff'].notna()]\n",
    "\n",
    "#return predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-02T15:27:03.250Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate(d_tree):\n",
    "    d_tree = predicted_df.copy()\n",
    "    event_real = np.array(d_tree['Next_Event'])  # taking next event col. as an array\n",
    "    event_pred = np.array(d_tree['DTree_Event'])  # taking the predictions as an array\n",
    "\n",
    "    acc_tree = accuracy_score(event_real, event_pred)  # calculates the accuracy based on the both arrays\n",
    "    print('Accuracy for event prediction DECISION TREE: {}%'.format(round(acc_tree, 2) * 100))\n",
    "\n",
    "    # RMSE D Tree\n",
    "    time_real_tree = np.array(d_tree['DTree_Actual_TimeDiff'])\n",
    "    time_pred_tree = np.array(d_tree['DTree_TimeDiff'])\n",
    "\n",
    "    rms_tree = sqrt(mean_squared_error(time_real_tree, time_pred_tree))/60/60/24\n",
    "\n",
    "    return print('Root mean squared error for time difference prediction DECISION TREE: {}'.format(round(rms_tree, 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-02T15:27:03.998Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(predicted_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "432px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
